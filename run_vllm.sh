# VLLM_USE_FLASHINFER_MOE_FP4=1 \
# VLLM_USE_FLASHINFER_MOE_FP8=1 \
# VLLM_FLASHINFER_MOE_BACKEND=latency \
# VLLM_USE_DEEP_GEMM=0 \
# VLLM_USE_TRTLLM_ATTENTION=0 \
# VLLM_ATTENTION_BACKEND=FLASH_ATTN \
CUDA_VISIBLE_DEVICES=1 \
vllm serve Alibaba-NLP/Tongyi-DeepResearch-30B-A3B
# vllm serve RESMP-DEV/Qwen3-Next-80B-A3B-Thinking-NVFP4
# VLLM_ATTENTION_BACKEND=CUTLASS_MLA \